{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "reading all libraries and packages that we need\n",
    "'''\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from skimage import io\n",
    "import pandas as pd\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.measure import regionprops, label\n",
    "import SimpleITK as sitk\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.morphology import closing, square, remove_small_objects, binary_erosion, disk, binary_dilation\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage.color import label2rgb\n",
    "import time\n",
    "\n",
    "from skimage.filters import  threshold_otsu, threshold_triangle, gaussian, threshold_local\n",
    "from skimage.morphology import convex_hull_image\n",
    "from skimage.draw import line, polygon\n",
    "import plotly\n",
    "from plotly.offline import plot \n",
    "from scipy.stats import kstest\n",
    "from scipy.stats import ks_2samp\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "from joblib import Parallel, delayed\n",
    "import mrc\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_and_label_dapi(seg_dapi, debris_size, erosion_radius):\n",
    "    ''' Segments the dapi image.\n",
    "    Ipnuts:\n",
    "    \n",
    "    seg_dapi: 2-dimensional numpy array (segmented image array)\n",
    "    debris_size: size of the debris to be removed (in pixels)\n",
    "    erosion_radius: radius (in pixels) of the disk to be used for binary ersion to separate connected nuclei\n",
    "    \n",
    "    Outputs:\n",
    "    nuclear_mask: segmented nuclei image\n",
    "    nuclear_labels: labled nuclei\n",
    "    \n",
    "    '''\n",
    "    # remove small debris from the segmented imaage\n",
    "    seg_dapi_1 = remove_small_objects(seg_dapi, debris_size)\n",
    "    \n",
    "    seg_dapi_2 = binary_erosion(seg_dapi_1, disk(2))\n",
    "    \n",
    "    # fill the holes in the image\n",
    "    nuclear_mask = ndi.binary_fill_holes(seg_dapi_2)\n",
    "\n",
    "    \n",
    "#     nuclear_mask = remove_small_objects(nuclear_mask, min_area_to_keep_cell)\n",
    "    \n",
    "    # erode the image\n",
    "    eroded_mask = binary_erosion(nuclear_mask, disk(erosion_radius))\n",
    "\n",
    "    distance = ndi.distance_transform_edt(eroded_mask)\n",
    "    local_maxi = peak_local_max(distance, indices=False, footprint=np.ones((1, 1)),\n",
    "                                labels=nuclear_mask)\n",
    "    markers = ndi.label(local_maxi)[0]\n",
    "    nuclear_labels = watershed(-distance, markers, mask=nuclear_mask, connectivity=2)\n",
    "    \n",
    "    return nuclear_mask, nuclear_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_and_label_dapi(dapi_image, debris_size, min_area_to_keep_cell, erosion_radius, block_size = 251):\n",
    "    ''' Segments the dapi image.\n",
    "    Ipnuts:\n",
    "    \n",
    "    dapi_image: 2-dimensional numpy array (image array)\n",
    "    debris_size: size of the debris to be removed (in pixels)\n",
    "    erosion_radius: radius (in pixels) of the disk to be used for binary ersion to separate connected nuclei\n",
    "    \n",
    "    Outputs:\n",
    "    nuclear_mask: segmented nuclei image\n",
    "    nuclear_labels: labled nuclei\n",
    "    \n",
    "    '''\n",
    "#     temp_seg_1 = dapi_image > threshold_otsu(dapi_image)\n",
    "    # locad thresholding to segment the nuclei\n",
    "    adaptive_thresh = threshold_local(dapi_image, block_size = block_size)\n",
    "    seg_dapi = dapi_image > adaptive_thresh\n",
    "    \n",
    "#     seg_dapi = temp_seg_1*temp_seg_2\n",
    "    \n",
    "    # use Otsu method to segment the dapi image\n",
    "#     seg_dapi = dapi_image > threshold_otsu(dapi_image)\n",
    "\n",
    "    # fill the holes in the image\n",
    "    nuclear_mask = ndi.binary_fill_holes(seg_dapi)\n",
    "    \n",
    "    # remove small debris from the segmented imaage\n",
    "    seg_dapi = remove_small_objects(seg_dapi, debris_size)\n",
    "\n",
    "    \n",
    "    \n",
    "    nuclear_mask = remove_small_objects(nuclear_mask, min_area_to_keep_cell)\n",
    "    \n",
    "    # erode the image\n",
    "    eroded_mask = binary_erosion(nuclear_mask, disk(erosion_radius))\n",
    "\n",
    "    distance = ndi.distance_transform_edt(eroded_mask)\n",
    "    local_maxi = peak_local_max(distance, indices=False, footprint=np.ones((1, 1)),\n",
    "                                labels=nuclear_mask)\n",
    "    markers = ndi.label(local_maxi)[0]\n",
    "    nuclear_labels = watershed(-distance, markers, mask=nuclear_mask, connectivity=2)\n",
    "    \n",
    "    return nuclear_mask, nuclear_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_big_cells(labeled_img, area_thresh, cir_thresh):\n",
    "    ''' Determining the package of the cells as a big cell.\n",
    "    Ipnuts:\n",
    "    \n",
    "    labeled_img: nuclei label from the labling function\n",
    "    area_thresh: maximum size of a nucleus (in pixels) to be considered as a single nucleus\n",
    "    cir_thresh: circularity threshold to see whether it is a single nucleus or multiple nuclei\n",
    "    \n",
    "    Outputs:\n",
    "    big_cells: label of the cells that are satistfying the area AND circularity threshold\n",
    "    \n",
    "    '''\n",
    "    circ = lambda r: (4 * np.pi * r.area) / (r.perimeter * r.perimeter)\n",
    "    big_cells = [(prop.label, prop.area, circ(prop)) for prop in regionprops(labeled_img)\n",
    "                 if prop.area > area_thresh and circ(prop) < cir_thresh]\n",
    "    return big_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cells(label_rem, image):\n",
    "    ''' Splitting a package of the cells\n",
    "    Ipnuts:\n",
    "    \n",
    "    labeled_rem: label of the object that should be splitted\n",
    "    image: image: 2D labeled image\n",
    "    \n",
    "    Outputs:\n",
    "    labels_rem: label of the cells that are going to split\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    relabels, tmp_img = np.zeros_like(image), np.zeros_like(image)\n",
    "    pts = []\n",
    "    row_cords =[]\n",
    "    col_cords = []\n",
    "    for prop in regionprops(label_rem):\n",
    "        x,y = np.int16(np.round(prop.centroid))\n",
    "        row_cords.append(x)\n",
    "        col_cords.append(y)\n",
    "        pts.extend([x,y])\n",
    "    if len(regionprops(label_rem))>2:\n",
    "        rr, cc = polygon(row_cords, col_cords)\n",
    "    else:\n",
    "        rr, cc = line(pts[0], pts[1], pts[2], pts[3])\n",
    "\n",
    "    tmp_img[rr,cc] =1\n",
    "    tmp_img = binary_dilation(tmp_img, disk(10))\n",
    "    tmp_img = convex_hull_image(tmp_img)\n",
    "    split_img = np.logical_and(image, np.logical_not(tmp_img))\n",
    "    distance_rem = ndi.distance_transform_edt(split_img)\n",
    "    local_maxi_rem = peak_local_max(distance_rem, indices=False, footprint=np.ones((1, 1)),\n",
    "                                labels=image)\n",
    "    markers_rem = label(local_maxi_rem, connectivity=2)\n",
    "    labels_rem = watershed(-distance_rem, markers_rem, mask=image, connectivity=2)\n",
    "    return labels_rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_final_labels_after_splitting_big_objects(nuclear_labels, big_cells, \n",
    "                                                    ero_rad, min_rem_area, min_area_to_keep_cell):\n",
    "    '''\n",
    "    Inputs:\n",
    "    nuclear_labels: 2D numpy labeled dapi image\n",
    "    big_cells: label of the big objects\n",
    "    ero_rad: radius of the disk to be used for eroading the region (convex hull - roi)\n",
    "    min_rem_area: removing any object less than of this size (in pixels)\n",
    "    min_area_to_keep_cell: minimum size of an acceptable nucleus (in mpixels)\n",
    "    \n",
    "    Outputs:\n",
    "    final_label: final label of nuecleus after removing small objects and splitting big packages\n",
    "    '''\n",
    "    \n",
    "    relabels = np.zeros_like(nuclear_labels)\n",
    "\n",
    "    for ii, area, cir in big_cells:\n",
    "        image = nuclear_labels==ii\n",
    "        chull = convex_hull_image(image)\n",
    "        rem = np.logical_and(chull, np.logical_not(image))\n",
    "        eroded_rem = binary_erosion(rem, disk(ero_rad))\n",
    "        eroded_rem = remove_small_objects(eroded_rem, min_rem_area)\n",
    "        label_rem = label(eroded_rem)\n",
    "\n",
    "        if np.max(label_rem) >= 2:\n",
    "            relabels = label(relabels + split_cells(label_rem, image))## split cells\n",
    "\n",
    "    assigned_relabels = np.zeros_like(relabels)\n",
    "    for p in regionprops(relabels):\n",
    "        if p.label > 0:\n",
    "            assigned_relabels[np.where(relabels==p.label)] = np.max(nuclear_labels) + p.label\n",
    "\n",
    "    label_img = label(nuclear_labels + assigned_relabels)\n",
    "    label_img = remove_small_objects(label_img, min_area_to_keep_cell)\n",
    "#     final_labels = clear_border(label_img)\n",
    "    final_labels = label_img\n",
    "    return final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir_name):\n",
    "    \n",
    "    if not os.path.exists(dir_name):\n",
    "            os.makedirs(dir_name)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = r\"D:\\12-January21\\GREB1\\FVWAE2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpaths = [os.path.join(root_path,f) for f in os.listdir(root_path) if f.endswith('.dv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Values\n",
    "sigma = 1\n",
    "erosion_radius = 31\n",
    "debris_size = 100\n",
    "min_area_to_keep_cell = 3000\n",
    "block_size = 651\n",
    "dilation_radius = 100\n",
    "area_thresh =20000# maximum area of the cell to be considered one cell\n",
    "cir_thresh = 0.70\n",
    "ero_rad = 5 # 5 for DV, radius of the disk to be used for eroading the region (convex hull - roi)\n",
    "min_rem_area = 2 # 2 for DV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_dir = os.path.join(root_path, 'Segmentation_erosion_radius_'+ str(erosion_radius)\n",
    "                       + '_min_area_' + str(min_area_to_keep_cell) +'_area_thresh_' + str(area_thresh))\n",
    "\n",
    "if not os.path.exists(seg_dir):\n",
    "    os.makedirs(seg_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating folders for the processed images\n",
    "raw_img_dir = os.path.join(os.path.dirname(seg_dir), 'raw')\n",
    "raw_img_dir_leave_top_bottom = os.path.join(os.path.dirname(seg_dir), 'raw_projected')\n",
    "seg_dapi_dir = os.path.join(os.path.dirname(seg_dir), 'seg_dapi')\n",
    "for dir_name in [raw_img_dir, raw_img_dir_leave_top_bottom, seg_dapi_dir]:\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame()\n",
    "for fpath in fpaths:\n",
    "    fname = os.path.basename(fpath)\n",
    "    main_fname = ('_').join(fname.split('_')[0:])\n",
    "    if 'wash' in fname:\n",
    "        fname_noext = os.path.splitext(fname)[0]\n",
    "        year, mon_date,  gene,_,_,_,time,field, _, _ =fname_noext.split('_')\n",
    "        # Store features\n",
    "        features = features.append([{'filename': main_fname,\n",
    "                                         'filepath': fpath,\n",
    "                                         'date': ('').join([year,mon_date]),\n",
    "                                         'gene': gene,\n",
    "                                         'time': time,\n",
    "                                         'field' : field,\n",
    "                                          },])\n",
    "    else:\n",
    "        fname_noext = os.path.splitext(fname)[0]\n",
    "        year, mon_date, gene,_,field,_,_ =fname_noext.split('_')\n",
    "        # Store features\n",
    "        features = features.append([{'filename': main_fname,\n",
    "                                         'filepath': fpath,\n",
    "                                         'date': ('').join([year,mon_date]),\n",
    "                                         'gene': gene,\n",
    "                                         'time': '0',\n",
    "                                         'field' : field,\n",
    "                                          },])\n",
    "\n",
    "features = features.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_projection(path,df):\n",
    "    fname = os.path.splitext(os.path.basename(path))[0] + '.tif'\n",
    "    \n",
    "    df = df[df['filepath'] == path]\n",
    "\n",
    "#     print(df['filepath'].values[0])\n",
    "    img_array = mrc.imread(path)\n",
    "    DIM=img_array.shape[1]\n",
    "#     print(img_array.shape)\n",
    "\n",
    "    #############################\n",
    "    dapi_image = np.max(img_array[0], axis=0)\n",
    "    io.imsave(os.path.join(raw_img_dir, 'dapi_' + fname), dapi_image)\n",
    "    \n",
    "    dapi_image = np.max(img_array[0, 2:DIM-1], axis=0)\n",
    "    io.imsave(os.path.join(raw_img_dir_leave_top_bottom, 'dapi_' + fname), dapi_image)\n",
    "    \n",
    "    exon_image = np.max(img_array[1], axis=0)\n",
    "    io.imsave(os.path.join(raw_img_dir,'exon_' + fname), exon_image)\n",
    "    \n",
    "    exon_image = np.max(img_array[1, 2:DIM-1], axis=0)\n",
    "    io.imsave(os.path.join(raw_img_dir_leave_top_bottom,'exon_' + fname), exon_image)\n",
    "    \n",
    "    intron_image = np.max(img_array[2], axis=0)\n",
    "    io.imsave(os.path.join(raw_img_dir,'intron_' + fname), intron_image)\n",
    "    \n",
    "    intron_image = np.max(img_array[2, 2:DIM-1], axis=0)\n",
    "    io.imsave(os.path.join(raw_img_dir_leave_top_bottom,'intron_' + fname), intron_image)\n",
    "#     dapi_image =  io.imread(os.path.join(fpath, fname))\n",
    "    # get the nuclear mask and nuclear labels\n",
    "    nuclear_mask, nuclear_labels = segment_and_label_dapi(dapi_image, debris_size, \n",
    "                                                          min_area_to_keep_cell,\n",
    "                                                          erosion_radius,\n",
    "                                                         block_size = block_size)\n",
    "\n",
    "    # get the big cells (to be split if it contains more than 1 cells)\n",
    "    big_cells = obtain_big_cells(nuclear_labels, area_thresh, cir_thresh)\n",
    "\n",
    "    # get the final labels after splitting the big cells\n",
    "    final_labels = obtain_final_labels_after_splitting_big_objects(nuclear_labels, big_cells, ero_rad, \n",
    "                                                min_rem_area, min_area_to_keep_cell)\n",
    "    # nuclei not touching border\n",
    "    non_border_labels = clear_border(final_labels)\n",
    "    nuc_mask = final_labels > 0\n",
    "    marked_dapi = mark_boundaries(dapi_image, non_border_labels, color=(1, 1, 1), outline_color=(1, 1, 1))\n",
    "    io.imsave(os.path.join(seg_dapi_dir, 'dapi_' + fname), np.uint8(marked_dapi*255))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "project=Parallel(n_jobs=8)(delayed(image_projection)(path,features) for path in features['filepath'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "newfpaths = [os.path.join(raw_img_dir_leave_top_bottom,f) for f in os.listdir(raw_img_dir_leave_top_bottom) if f.endswith('.tif')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segmenting DAPI channel\n",
    "alpha = 40\n",
    "\n",
    "for fpath in newfpaths:\n",
    "    if 'dapi' in fpath:\n",
    "        \n",
    "    #     print(fpath)\n",
    "        dapi_image = io.imread(fpath)\n",
    "        f = dapi_image\n",
    "        \n",
    "        blurred_f = ndi.gaussian_filter(f, 3)\n",
    "\n",
    "        filter_blurred_f = ndi.gaussian_filter(blurred_f, 1)\n",
    "\n",
    "        \n",
    "        sharpened = blurred_f + alpha * (blurred_f - filter_blurred_f)\n",
    "\n",
    "        filled_seg_dapi = ndi.binary_fill_holes(dapi_image > (0.6*threshold_otsu(dapi_image)))\n",
    "\n",
    "        adaptive_thresh = threshold_local(sharpened, block_size = block_size)\n",
    "\n",
    "        temp_seg_2 = ndi.binary_fill_holes (sharpened > adaptive_thresh)\n",
    "        \n",
    "        seg_dapi = np.logical_and(filled_seg_dapi, temp_seg_2)\n",
    "\n",
    "        fname = os.path.basename(fpath)\n",
    "\n",
    "        io.imsave(os.path.join(seg_dapi_dir, 'seg_' + fname), np.uint8(255*seg_dapi))\n",
    "#         io.imsave(os.path.join(seg_dapi_dir, 'seg_2_' + fname), np.uint8(255*temp_seg_2))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "newfeatures = pd.DataFrame()\n",
    "for fpath in newfpaths:\n",
    "    fname = os.path.basename(fpath)\n",
    "    main_fname = ('_').join(fname.split('_')[1:])\n",
    "    if 'wash' in fname:\n",
    "        fname_noext = os.path.splitext(fname)[0]\n",
    "        wavelength, year, mon_date, _,_,_,_,time,field, _, _ =fname_noext.split('_')\n",
    "        # Store features\n",
    "        newfeatures = newfeatures.append([{'filename': main_fname,\n",
    "                                         'filepath': fpath,\n",
    "                                         'date': ('').join([year,mon_date]),\n",
    "                                         'time': time,\n",
    "                                         'field' : field,\n",
    "                                     'wavelength' : wavelength,\n",
    "                                          },])\n",
    "    else:\n",
    "        fname_noext = os.path.splitext(fname)[0]\n",
    "        wavelength, year, mon_date, _,_, field, _, _ =fname_noext.split('_')\n",
    "        # Store features\n",
    "        newfeatures = newfeatures.append([{'filename': main_fname,\n",
    "                                         'filepath': fpath,\n",
    "                                         'date': ('').join([year,mon_date]),\n",
    "                                         'time': '0',\n",
    "                                         'field' : field,\n",
    "                                     'wavelength' : wavelength,\n",
    "                                          },])\n",
    "\n",
    "newfeatures = newfeatures.reset_index(drop=True)\n",
    "newfeatures = newfeatures.astype({\"field\": int})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = {'15min': 15, '30min':30, '45min': 45, '60min': 60, '75min': 75, '90min':90,'0':0}\n",
    "newfeatures[\"time\"] = newfeatures[\"time\"].map(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_dir = os.path.join(root_path,  'Segmentation_erosion_radius_'+ str(erosion_radius)\n",
    "                       + '_min_area_' + str(min_area_to_keep_cell) +'_area_thresh_' + str(area_thresh))\n",
    "\n",
    "if not os.path.exists(seg_dir):\n",
    "    os.makedirs(seg_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_segmentation(filename,feat):\n",
    "    summary = {}\n",
    "\n",
    "    df = feat[feat['filename'] == filename]\n",
    "    t=df.iloc[0]['time']#str(df.iloc[0]['time']) + \"_\" + df.iloc[0]['treatment']\n",
    "    f=df.iloc[0]['field']\n",
    "    summary['time' +str(t)+'field'+str(f)]={}\n",
    "    for channel in df['wavelength']:\n",
    "        sdf = df[df['wavelength']==channel]\n",
    "        fname = os.path.basename(list(sdf['filepath'])[0])\n",
    "        \n",
    "        ## Segmenting the nuclei in DAPI Channel\n",
    "        \n",
    "        if channel == 'dapi':\n",
    "            seg_name = 'seg_' + fname\n",
    "            seg_dapi =  io.imread(os.path.join(seg_dapi_dir, seg_name))\n",
    "             # get the nuclear mask and nuclear labels\n",
    "            nuclear_mask, nuclear_labels = prune_and_label_dapi(seg_dapi, debris_size, erosion_radius)\n",
    "\n",
    "             # get the big cells (to be split if it contains more than 1 cells)\n",
    "            big_cells = obtain_big_cells(nuclear_labels, area_thresh, cir_thresh)\n",
    "\n",
    "            # get the final labels after splitting the big cells\n",
    "            final_labels_1 = obtain_final_labels_after_splitting_big_objects(nuclear_labels, big_cells, ero_rad, \n",
    "                                                                min_rem_area, min_area_to_keep_cell)\n",
    "\n",
    "            nuc_mask = final_labels_1\n",
    "\n",
    "            # dialte the nuclear mask to get back to right size\n",
    "            temp_img = np.logical_or(binary_dilation(nuc_mask, disk(2)), binary_fill_holes(seg_dapi))\n",
    "\n",
    "            # use watershed method with final_labels as markers to obtain labeled cell mask\n",
    "            final_labels = watershed(temp_img, markers=final_labels_1, mask=temp_img)\n",
    "\n",
    "            # nuclei not touching border\n",
    "            non_border_labels = clear_border(final_labels)\n",
    "            \n",
    "\n",
    "        ## Segmenting the exonic spots in exon Channel\n",
    "\n",
    "\n",
    "        elif channel == 'exon':\n",
    "            exon_image =  io.imread(os.path.join(fpath, fname))\n",
    "            seg_exon = exon_image > (threshold_otsu(exon_image))\n",
    "            \n",
    "        \n",
    "        ## Segmenting the intronic spots in intron Channel\n",
    "\n",
    "                \n",
    "        elif channel == 'intron':\n",
    "            intron_image = sitk.ReadImage(os.path.join(fpath, fname))\n",
    "            gaussian_blur = sitk.SmoothingRecursiveGaussianImageFilter()\n",
    "            gaussian_blur.SetSigma ( float ( sigma ) )\n",
    "            blur_intron = gaussian_blur.Execute ( intron_image )\n",
    "            max_entropy_filter = sitk.MaximumEntropyThresholdImageFilter()\n",
    "            max_entropy_filter.SetInsideValue(0)\n",
    "            max_entropy_filter.SetOutsideValue(1)\n",
    "            seg = max_entropy_filter.Execute(blur_intron)\n",
    "            seg_intron = sitk.GetArrayFromImage(seg)\n",
    "            blur_intron_img = sitk.GetArrayFromImage(blur_intron)\n",
    "            intron_threshold = np.min(blur_intron_img[np.where(seg_intron != 0)])\n",
    "            seg_intron = blur_intron_img > intron_threshold\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            print('Different than dapi, exon or intron image found!')\n",
    "            \n",
    "    ##Saving the segmentation of all three channels\n",
    "                \n",
    "    comb_img = np.zeros((seg_exon.shape[0], seg_exon.shape[0],3))\n",
    "    comb_img[:,:,0] = seg_intron\n",
    "    comb_img[:,:,1] = seg_exon\n",
    "    comb_img[:,:,2] = nuc_mask\n",
    "    marked_img = mark_boundaries(comb_img, non_border_labels, color=(1, 1, 1), outline_color=(1, 1, 1))\n",
    "    io.imsave(os.path.join(seg_dir, 'combined_' + fname), np.uint8(marked_img*255))\n",
    "    \n",
    "    \n",
    "    ## Label all the spots in three channels\n",
    "    \n",
    "    green=label(seg_exon)\n",
    "    red=label(seg_intron)\n",
    "    blue=label(non_border_labels)\n",
    "        \n",
    "\n",
    "\n",
    "    ### get measurements\n",
    "    \n",
    "    ## First, for each nucleus within the DAPI channel we find the exonic spots that are inside that nucleus\n",
    "\n",
    "    for nuc in regionprops(blue):\n",
    "        nuc_id=nuc.label\n",
    "        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]={}\n",
    "        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['intron']=[]\n",
    "        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['exon']=[]\n",
    "        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['colocspot']=[]\n",
    "        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['Ncolocspot']=[]\n",
    "        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['nascent']=[]\n",
    "        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['Nnascent']=[]\n",
    "\n",
    "        for exon in regionprops(green,intensity_image=exon_image):\n",
    "            exon_id=exon.label\n",
    "            if (((exon.coords[:, None] == nuc.coords).all(-1).any(-1)==True).any()):\n",
    "                summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['exon'].append(exon_id)\n",
    "                \n",
    "        ## Second, for each intronic spots inside the nucleus, we search for any colocation with exonic sports (nascent mRNA)\n",
    "\n",
    "        for intron in regionprops(red,intensity_image=sitk.GetArrayFromImage(intron_image)):\n",
    "            intron_id=intron.label\n",
    "            if (((intron.coords[:, None] == nuc.coords).all(-1).any(-1)==True).all()):\n",
    "                summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['intron'].append(intron_id)         \n",
    "\n",
    "                for nucexon in summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['exon']:\n",
    "                    if (((intron.coords[:, None] == regionprops(green)[nucexon-1].coords).all(-1).any(-1)==True).any()):\n",
    "                        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['colocspot'].append([intron_id,nucexon])\n",
    "                        \n",
    "        ## Finally, in some occasion one intronic spot has colocation with two (or more) exonic spots or\n",
    "        ## one exonic spots has colocation with two (or more) intronic spots,\n",
    "        ## we are going to set them all equally and consider them just as \"one nascent mRNA\"\n",
    "        ## or \"one burst\"\n",
    "        X=np.array(summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['colocspot'])\n",
    "        W= np.array(summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['intron'])\n",
    "        Y=np.array(summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['exon'])\n",
    "\n",
    "        if(len(X)>0):\n",
    "            if((len(X[:,0])>len(np.unique(X[:,0]))) or(len(X[:,1])>len(np.unique(X[:,1])))) :\n",
    "                unique, counts = np.unique(X[:,0], return_counts=True)\n",
    "                unique2, counts2 = np.unique(X[:,1], return_counts=True)\n",
    "                if((len(X[:,0])>len(np.unique(X[:,0])))):\n",
    "                    for i in (unique[(counts)>1]):\n",
    "                        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['nascent'].append([i,X[X[:,0]==i][:,1]])\n",
    "                    v=([X[X[:,0]==i][0][0] for i in unique[(counts)==1]])\n",
    "                    u=([X[X[:,0]==i][0][1] for i in unique[(counts)==1]])\n",
    "                    r=([X[X[:,0]==i][0] for i in unique[(counts)==1]])\n",
    "                    if (len(u)>len(np.unique(u))):\n",
    "                        for j in np.unique(u):\n",
    "                            summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['nascent'].append([X[X[:,1]==j][:,0],j])\n",
    "                    elif(len(r)>0):\n",
    "                        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['nascent']=np.vstack((summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['nascent'],r))\n",
    "                else:\n",
    "                    for k in (unique2):\n",
    "                        summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['nascent'].append([X[X[:,1]==k][:,0],k]) \n",
    "            else:\n",
    "                summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['nascent']=summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['colocspot']\n",
    "        else:\n",
    "            summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['Nnascent']=len(summary['time' +str(t)+'field'+str(f)]['nuc'+str(nuc_id)]['nascent'])   \n",
    "    with open(os.path.join(seg_dir, 'summary_t_'+str(t)+'_f_'+str(f)+'.pickle'), 'wb') as handle:\n",
    "        pickle.dump(summary, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 34455.50477766991\n"
     ]
    }
   ],
   "source": [
    "fpath = raw_img_dir_leave_top_bottom\n",
    "from time import time\n",
    "begin=time()\n",
    "\n",
    "Parallel(n_jobs=2)(delayed(image_segmentation)(filename,newfeatures) for filename in newfeatures['filename'].unique())\n",
    "        \n",
    "print(\"Total time:\",time()-begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
